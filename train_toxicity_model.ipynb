{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "vYpMuI4YUG0u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# Execution parameters\n",
        "#\n",
        "\n",
        "FILES_PATH=\"preprocessed_datasets/\"\n",
        "DATASET_NAME=\"coronavirus_2021q1_all_preprocessed\"\n",
        "BATCH_SIZE=2**14\n",
        "TRAIN_TEST_METHOD=\"controlled_both\" # random, controlled_users, controlled_subreddits, controlled_both\n",
        "MODEL_ARCHITECTURE=\"toxicity_simple\" # toxicity_simple, toxicity_NCF, toxicity_BOW, toxicity_BERT_simple, toxicity_BERTwithembeddings\n",
        "DROP_UNTESTED_USERS= False\n",
        "TRAIN_WITH_GPU = True\n",
        "BALANCE_TRAIN_USERS = True\n",
        "TRAINING_GOAL=\"regression\" #regression, classification\n",
        "\n",
        "# Grid search parameters\n",
        "learning_rates=[1e-3,1e-4,1e-5]\n",
        "n_factors=[32,128,512]\n",
        "l2_reg=[5e-4,1e-5,0]\n",
        "\n",
        "epochs=100\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and TRAIN_WITH_GPU else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "aMmikBfiUG0v",
        "outputId": "72829fda-0f7a-430b-c494-afcb1a256da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxic interactions: 9.35%\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Read dataset and obtain the basic model inputs and outputs\n",
        "#\n",
        "\n",
        "df=pd.read_csv(f'{FILES_PATH}/{DATASET_NAME}_preprocessed_toxicity.csv',encoding='UTF_8')\n",
        "\n",
        "#We roughly defined a (user,subreddit) interaction as toxic when the mean toxicity is >0\n",
        "df=df.groupby(['author_id','subreddit_id'],as_index=False)['Toxicity'].mean()\n",
        "if TRAINING_GOAL==\"classification\":\n",
        "    df['Toxicity']=df['Toxicity'].apply(lambda x: 0 if x<0 else 1)\n",
        "\n",
        "print(f\"Toxic interactions: {(100*(df[df['Toxicity']>0].shape[0]/len(df['Toxicity']))):.2f}%\") #Percentage of toxic interactions (not comments!)\n",
        "\n",
        "#Primary inputs (author_id, subreddit_id) and outputs (toxicity)\n",
        "x=df.drop(['Toxicity'],axis=1).to_numpy().astype(int)\n",
        "y=df['Toxicity'].to_numpy()\n",
        "\n",
        "n_users=df['author_id'].nunique()\n",
        "n_subreddits=df['subreddit_id'].nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# Data analysis and plots\n",
        "#\n",
        "\n",
        "if TRAINING_GOAL==\"classification\":\n",
        "\n",
        "    plt.rcParams[\"figure.figsize\"]=(3,3)\n",
        "\n",
        "    #Plot histogram of Avg. Toxicity per user\n",
        "    avg_toxicity_per_user=df.groupby(['author_id'],as_index=False)['Toxicity'].mean()\n",
        "    avg_toxicity_per_user['Toxicity'].plot.hist()\n",
        "    plt.show()\n",
        "\n",
        "    #Obtain dataframe with (user_id,comment_count,mean_toxicity)\n",
        "    no_comments_per_user=df.groupby(['author_id'],as_index=False).size()\n",
        "    no_comments_per_user[\"mean_toxicity\"]=avg_toxicity_per_user[\"Toxicity\"]\n",
        "\n",
        "    #** Obtain dataframe with (number_of_comments, avg toxicity for users with number_of_comments)\n",
        "    avg_toxicity_per_interaction_count=no_comments_per_user.groupby(['size'],as_index=False)['mean_toxicity'].mean()\n",
        "\n",
        "    #Obtain dataframe with (number_of_comments, number of users with number_of_comments comments)\n",
        "    no_users_per_comment_count=no_comments_per_user.rename(columns={'size':'comment_count'}).groupby(['comment_count'],as_index=False).size()\n",
        "\n",
        "    #Obtain regression for **\n",
        "    m, b = np.polyfit(no_comments_per_user[\"size\"].to_numpy(), avg_toxicity_per_user['Toxicity'].to_numpy(), 1)\n",
        "\n",
        "    #Plot everything:\n",
        "\n",
        "    #Left y-axis\n",
        "    plt.plot(no_comments_per_user[\"size\"].to_numpy(), avg_toxicity_per_user['Toxicity'].to_numpy(), 'o', alpha=0.005, c='black',label=\"Individual user data points\\n(no. of interactions, avg. toxicity)\")\n",
        "    plt.plot(avg_toxicity_per_interaction_count[\"size\"].to_numpy(), avg_toxicity_per_interaction_count['mean_toxicity'].to_numpy(), 'o', alpha=.8, c='red', label=\"Avg. toxicity of users with n interactions\")\n",
        "    plt.plot(no_comments_per_user[\"size\"].to_numpy(), m*no_comments_per_user[\"size\"].to_numpy() + b)\n",
        "\n",
        "    plt.ylim((0,1))\n",
        "    plt.xlim((0,25))\n",
        "\n",
        "    plt.xlabel(\"(user,subreddit) interactions\")\n",
        "    plt.ylabel(\"Average interaction toxicity of user\")\n",
        "\n",
        "    plt.legend(bbox_to_anchor=(1.3, 1), loc=\"upper left\")\n",
        "\n",
        "    #Switch axis\n",
        "    plt.twinx()\n",
        "\n",
        "    #Right y-axis\n",
        "    plt.plot(no_users_per_comment_count[\"comment_count\"].to_numpy(),no_users_per_comment_count[\"size\"].to_numpy(),color='green',label=\"No. of users per interaction count\")\n",
        "\n",
        "    plt.ylim((0,3000))\n",
        "\n",
        "    plt.ylabel(\"No. of Users\")\n",
        "\n",
        "    plt.legend(bbox_to_anchor=(1.3, 0.7), loc=\"upper left\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "JqzSJzbdUG0x",
        "outputId": "846b0901-0dd4-40c4-ccde-2d046c6c9d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1125 users  and 259 that meet criteria\n",
            "Intersecting these users and subreddits, 1125 and 255 are preserved\n",
            "Total test samples: 2250\n",
            "Applying a weight of 1.00 for positive samples in training loss\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Define and perform train/test split, generate datasets\n",
        "#\n",
        "\n",
        "def train_test_split(df):\n",
        "    user_groups=df.groupby('author_id')\n",
        "    subreddit_groups=df.groupby('subreddit_id')\n",
        "\n",
        "    test=[]\n",
        "\n",
        "    # Approach \"random\": If the user posts in more than 10 subreddits, use them for training set (10% of their interactions)\n",
        "    if TRAIN_TEST_METHOD==\"random\":\n",
        "        for _,group in user_groups:\n",
        "            if group.shape[0]>=10:\n",
        "                test+=(group.sample(n=int(group.shape[0]*0.15)).to_dict(orient=\"records\"))\n",
        "\n",
        "    # Approach \"controlled\": If they post in more than 10 subreddits AND have a\n",
        "    elif TRAIN_TEST_METHOD==\"controlled_users\":\n",
        "        for _,group in user_groups:\n",
        "            if group.shape[0]>=10 and (group[\"Toxicity\"]>0).sum()>1 and ((group[\"Toxicity\"].shape[0]-(group[\"Toxicity\"]>0).sum())>1):\n",
        "                test += (group[group[\"Toxicity\"]==1].sample(1).to_dict(orient=\"records\"))\n",
        "                test += (group[group[\"Toxicity\"]==0].sample(1).to_dict(orient=\"records\"))\n",
        "\n",
        "    elif TRAIN_TEST_METHOD==\"controlled_subreddits\":\n",
        "        for _,group in subreddit_groups:\n",
        "            if group.shape[0]>=10 and (group[\"Toxicity\"]>0).sum()>1 and ((group[\"Toxicity\"].shape[0]-(group[\"Toxicity\"]>0).sum())>1):\n",
        "                test += (group[group[\"Toxicity\"]>1].sample(1).to_dict(orient=\"records\"))\n",
        "                test += (group[group[\"Toxicity\"]<=0].sample(1).to_dict(orient=\"records\"))\n",
        "\n",
        "    elif TRAIN_TEST_METHOD==\"controlled_both\":\n",
        "        \n",
        "        valid_users=[]\n",
        "        valid_subreddits=[]\n",
        "        for user,group in user_groups:\n",
        "            if group.shape[0]>=10 and (group[\"Toxicity\"]>0).sum()>2 and ((group[\"Toxicity\"].shape[0]-(group[\"Toxicity\"]>0).sum())>2):\n",
        "                valid_users.append(user)\n",
        "\n",
        "\n",
        "        for subreddit,group in subreddit_groups:\n",
        "            if group.shape[0]>=10 and (group[\"Toxicity\"]>0).sum()>2 and ((group[\"Toxicity\"].shape[0]-(group[\"Toxicity\"]>0).sum())>2):\n",
        "                valid_subreddits.append(subreddit)\n",
        "        \n",
        "        print(f\"Found {len(valid_users)} users  and {len(valid_subreddits)} that meet criteria\")\n",
        "\n",
        "        valid_rows = df[(df[\"author_id\"].isin(valid_users)) & (df[\"subreddit_id\"].isin(valid_subreddits))]\n",
        "\n",
        "        print(f\"Intersecting these users and subreddits, {valid_rows['author_id'].nunique()} and {valid_rows['subreddit_id'].nunique()} are preserved\")\n",
        "\n",
        "        test += (valid_rows[valid_rows[\"Toxicity\"]>0].sample(valid_rows[\"author_id\"].nunique()).to_dict(orient=\"records\"))\n",
        "        test += (valid_rows[valid_rows[\"Toxicity\"]<=0].sample(valid_rows[\"author_id\"].nunique()).to_dict(orient=\"records\"))\n",
        "\n",
        "    test = pd.DataFrame(test)\n",
        "    print(f\"Total test samples: {test.shape[0]}\")\n",
        "\n",
        "    train = pd.concat([df, test]).drop_duplicates(keep=False)\n",
        "    \n",
        "    # Unccomment for oversampling toxic interactions in train set\n",
        "    # train = pd.concat([train,pd.concat([train[train[\"Toxicity\"]==1]]*(round(toxic_labels_weight)-1))])\n",
        "    # toxic_labels_weight=(len(train['Toxicity'])-train['Toxicity'].sum())/train['Toxicity'].sum()\n",
        "    \n",
        "    if DROP_UNTESTED_USERS or BALANCE_TRAIN_USERS:\n",
        "        train=train[(train[\"author_id\"].isin(test[\"author_id\"])) & (train[\"subreddit_id\"].isin(test[\"subreddit_id\"]))]\n",
        "    if BALANCE_TRAIN_USERS:\n",
        "        newtrain=[]\n",
        "        user_groups=train.groupby('author_id')\n",
        "        for user,group in user_groups:\n",
        "            positives = (group[\"Toxicity\"]>0).sum()\n",
        "            negatives = group[\"Toxicity\"].shape[0]-(group[\"Toxicity\"]>0).sum()\n",
        "            newtrain += group[group[\"Toxicity\"]>0].sample(min(positives,negatives)).to_dict(orient=\"records\")\n",
        "            newtrain += group[group[\"Toxicity\"]<=0].sample(min(positives,negatives)).to_dict(orient=\"records\")\n",
        "\n",
        "\n",
        "        train = pd.DataFrame(newtrain)\n",
        "        train = train.sample(train.shape[0]*10,replace=True)\n",
        "    return train, test\n",
        "\n",
        "#Split train and test sets\n",
        "\n",
        "train,test=train_test_split(df) \n",
        "\n",
        "toxic_labels_weight=(len(train['Toxicity'])-(train[\"Toxicity\"]>0).sum())/(train[\"Toxicity\"]>0).sum()\n",
        "print(f\"Applying a weight of {toxic_labels_weight:.2f} for positive samples in training loss\")\n",
        "\n",
        "X_train=train.drop(['Toxicity'],axis=1).to_numpy().astype(int)\n",
        "X_test=test.drop(['Toxicity'],axis=1).to_numpy().astype(int)\n",
        "\n",
        "y_train=train['Toxicity'].to_numpy()\n",
        "y_test=test['Toxicity'].to_numpy()\n",
        "\n",
        "X_train, X_test = torch.Tensor(X_train).int().to(device), torch.Tensor(X_test).int().to(device)\n",
        "y_train, y_test = torch.Tensor(y_train).float().to(device), torch.Tensor(y_test).float().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "9n0G9UlvUG0x"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Load additional data depending on architecture\n",
        "#\n",
        "\n",
        "\n",
        "if MODEL_ARCHITECTURE==\"toxicity_BOW\":\n",
        "\n",
        "    #Load users' and subreddits' Bag of Words\n",
        "    user_bows=pickle.load(open(\"preprocessed_datasets/coronavirus_2021q1_all_preprocessed_USERS_BAG_OF_WORDS\",\"rb\"))\n",
        "    subreddit_bows=pickle.load(open(\"preprocessed_datasets/coronavirus_2021q1_all_preprocessed_SUBREDDIT_BAG_OF_WORDS\",\"rb\"))\n",
        "\n",
        "    #We're gonna work with binary vectors for now\n",
        "    user_bows[user_bows>1]=1\n",
        "    subreddit_bows[subreddit_bows>1]=1\n",
        "\n",
        "    user_bows=torch.Tensor(user_bows).float().to(device)\n",
        "    subreddit_bows=torch.Tensor(subreddit_bows).float().to(device)\n",
        "\n",
        "if MODEL_ARCHITECTURE in [\"toxicity_BERT_simple\",\"toxicity_BERTwithembeddings\"]:\n",
        "    df_raw=pd.read_csv(f'{FILES_PATH}/{DATASET_NAME}_preprocessed_toxicity.csv',encoding='UTF_8')\n",
        "    \n",
        "    # train_combs = train[['author_id','subreddit_id']].apply(tuple,axis=1)\n",
        "    # raw_combs = df_raw[['author_id','subreddit_id']].apply(tuple,axis=1)\n",
        "\n",
        "    df_raw = df_raw[~(df_raw[['author_id','subreddit_id']].apply(tuple,axis=1).isin(test[['author_id','subreddit_id']].apply(tuple,axis=1)))]\n",
        "    embeddings=pickle.load(open(\"BERT_EMBEDDINGS/MARCH_21\",'rb'))\n",
        "    \n",
        "    print(n_users)\n",
        "    print(n_subreddits)\n",
        "\n",
        "    print(df_raw[\"author_id\"].nunique(),df_raw[\"subreddit_id\"].nunique())\n",
        "    commentlists_user=df_raw.groupby('author_id')['comment_id'].apply(list).reset_index(name=\"comment_ids\")\n",
        "    bertavg_users=np.stack(commentlists_user[\"comment_ids\"].apply(lambda x: np.average(embeddings[x,:],axis=0)).to_numpy(),axis=0)\n",
        "\n",
        "    commentlists_subreddit=df_raw.groupby('subreddit_id')['comment_id'].apply(list).reset_index(name=\"comment_ids\")\n",
        "    bertavg_subreddits=np.stack(commentlists_subreddit[\"comment_ids\"].apply(lambda x: np.average(embeddings[x,:],axis=0)).to_numpy(),axis=0)\n",
        "    \n",
        "    bertavg_users=torch.Tensor(bertavg_users).float().to(device)\n",
        "    bertavg_subreddits=torch.Tensor(bertavg_subreddits).float().to(device)\n",
        "\n",
        "    print(bertavg_subreddits.shape)\n",
        "    print(bertavg_users.shape)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "mo8bov0JUG0x"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Define the model architecture\n",
        "#\n",
        "\n",
        "class ToxicitySimple(nn.Module):\n",
        "    def __init__(self,d):\n",
        "        super(ToxicitySimple,self).__init__()\n",
        "\n",
        "        #We only have one trainable layer depth (two Embeddings and two FC's)\n",
        "\n",
        "        self.u = nn.Embedding(n_users,d)                #Embedding author_id -> n_factors\n",
        "        self.m = nn.Embedding(n_subreddits,d)           #Embedding subreddit_id-> n_factors\n",
        "        # self.u1 = nn.Linear(user_bows.shape[1],n_factors)       #FC        author bow -> n_factors\n",
        "        # self.m1 = nn.Linear(subreddit_bows.shape[1],n_factors)  #FC        subreddit bow -> n_factors\n",
        "\n",
        "        #Initialize weights\n",
        "        self.u.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.m.weight.data.uniform_(-0.1, 0.1)\n",
        "        # self.u1.weight.data.uniform_(-0.05, 0.05)\n",
        "        # self.m1.weight.data.uniform_(-0.05, 0.05)\n",
        "\n",
        "    def forward(self, x):\n",
        "        users, subreddits = x[:,0] , x[:,1] #Get author_id and subreddit_id from input\n",
        "        # ubows, sbows = user_bows[x[:,0].long()], subreddit_bows[x[:, 1].long()] #Get author bow and subreddit bow indirectly from input\n",
        "        u,m = self.u(users), self.m(subreddits) #Embed author and subreddit\n",
        "\n",
        "        #For simple model and BCEwithLogits\n",
        "        return (u*m).sum(1)\n",
        "\n",
        "#References: https://towardsdatascience.com/paper-review-neural-collaborative-filtering-explanation-implementation-ea3e031b7f96\n",
        "class ToxicityNCF(nn.Module):\n",
        "    def __init__(self,d):\n",
        "        super(ToxicityNCF,self).__init__()\n",
        "\n",
        "        #We only have one trainable layer depth (two Embeddings and two FC's)\n",
        "        self.u_mf = nn.Embedding(n_users,d)                #Embedding author_id -> n_factors\n",
        "        self.u_mlp = nn.Embedding(n_users,d)                #Embedding author_id -> n_factors\n",
        "        self.m_mf = nn.Embedding(n_subreddits,d)           #Embedding subreddit_id-> n_factors\n",
        "        self.m_mlp = nn.Embedding(n_subreddits,d)           #Embedding subreddit_id-> n_factors\n",
        "\n",
        "        #Initialize weights\n",
        "        self.u_mf.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.u_mlp.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "        self.m_mf.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.m_mlp.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "        self.fc_mf_1 = nn.Linear(d,d//2)\n",
        "        self.fc_mf_2 = nn.Linear(d//2,d//4)\n",
        "\n",
        "        self.fc_mf_1.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.fc_mf_2.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "        self.fc_mlp_1 = nn.Linear(d*2,d)\n",
        "        self.fc_mlp_2 = nn.Linear(d,d//2)\n",
        "        self.fc_mlp_3 = nn.Linear(d//2,d//4)\n",
        "\n",
        "        self.fc_mlp_1.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.fc_mlp_2.weight.data.uniform_(-0.5, 0.5)\n",
        "        self.fc_mlp_3.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "        self.neumf = nn.Linear(d//2,1)\n",
        "        self.neumf.weight.data.uniform_(-0.5, 0.5)\n",
        "\n",
        "        self.dropout=nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        users, subreddits = x[:,0] , x[:,1] #Get author_id and subreddit_id from input\n",
        "        # ubows, sbows = user_bows[x[:,0].long()], subreddit_bows[x[:, 1].long()] #Get author bow and subreddit bow indirectly from input\n",
        "        u_mf, m_mf = self.u_mf(users), self.m_mf(subreddits) #Embed author and subreddit\n",
        "\n",
        "        u_mlp, m_mlp = self.u_mlp(users), self.m_mlp(subreddits)\n",
        "\n",
        "        mf = self.dropout(nn.functional.relu(self.fc_mf_1(u_mf*m_mf)))\n",
        "        mf = self.dropout(nn.functional.relu(self.fc_mf_2(mf)))\n",
        "\n",
        "        mlp = self.dropout(nn.functional.relu(self.fc_mlp_1(torch.cat((u_mlp,m_mlp),1))))\n",
        "        mlp = self.dropout(nn.functional.relu(self.fc_mlp_2(mlp)))\n",
        "        mlp = self.dropout(nn.functional.relu(self.fc_mlp_3(mlp)))\n",
        "\n",
        "        neumf = self.neumf(torch.cat((mf,mlp),1))\n",
        "\n",
        "        return neumf\n",
        "\n",
        "class ToxicityBOW(nn.Module):\n",
        "    def __init__(self,d):\n",
        "        super(ToxicityBOW,self).__init__()\n",
        "\n",
        "        #We only have one trainable layer depth (two Embeddings and two FC's)\n",
        "\n",
        "        self.u = nn.Embedding(n_users,d)                #Embedding author_id -> n_factors\n",
        "        self.m = nn.Embedding(n_subreddits,d)           #Embedding subreddit_id-> n_factors\n",
        "        self.u1 = nn.Linear(user_bows.shape[1],d)       #FC        author bow -> n_factors\n",
        "        self.m1 = nn.Linear(subreddit_bows.shape[1],d)  #FC        subreddit bow -> n_factors\n",
        "\n",
        "        #Initialize weights\n",
        "        self.u.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.m.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.u1.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.m1.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        users, subreddits = x[:,0] , x[:,1] #Get author_id and subreddit_id from input\n",
        "        ubows, sbows = user_bows[x[:,0].long()], subreddit_bows[x[:, 1].long()] #Get author bow and subreddit bow indirectly from input\n",
        "        u,m = self.u(users), self.m(subreddits) #Embed author and subreddit\n",
        "        u1, m1 = self.u1(ubows), self.m1(sbows) #Reduce dimensionality of author bow and subreddit bow\n",
        "        \n",
        "        #Concat u with u1, and m with m1. Compute the dot product of the resulting vectors, and pass the value through a sigmoid.\n",
        "\n",
        "        #If using the model with BOW and BCE\n",
        "        return (torch.cat((u, u1), 1) * torch.cat((m, m1), 1)).sum(1).view(-1, 1)\n",
        "\n",
        "class ToxicityBERTSimple(nn.Module):\n",
        "    def __init__(self,d):\n",
        "        super(ToxicityBERTSimple,self).__init__()\n",
        "\n",
        "        #We only have one trainable layer depth (two Embeddings and two FC's)\n",
        "\n",
        "        self.u1 = nn.Linear(bertavg_users.shape[1],d)       #FC        author bow -> n_factors\n",
        "        self.m1 = nn.Linear(bertavg_subreddits.shape[1],d)  #FC        subreddit bow -> n_factors\n",
        "\n",
        "        self.u = nn.Embedding(n_users,d)                #Embedding author_id -> n_factors\n",
        "        self.m = nn.Embedding(n_subreddits,d)           #Embedding subreddit_id-> n_factors\n",
        "        \n",
        "        self.fc1 = nn.Linear(2*d, d//2)\n",
        "        self.fc2 = nn.Linear(d//2, 1)\n",
        "\n",
        "        #Initialize weights\n",
        "        # self.u1.weight.data.uniform_(-0.1, 0.1)\n",
        "        # self.m1.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ubert, sbert = bertavg_users[x[:,0].long()], bertavg_subreddits[x[:, 1].long()] #Get author bow and subreddit bow indirectly from input\n",
        "        u1, m1 = self.u1(ubert), self.m1(sbert) #Reduce dimensionality of author bow and subreddit bow\n",
        "\n",
        "        conc=torch.cat((u1,m1),1)\n",
        "        conc=F.dropout(F.relu(conc),0.1)\n",
        "        conc = F.dropout(F.relu(self.fc1(conc)),0.1)\n",
        "        conc = self.fc2(conc)\n",
        "\n",
        "        return conc\n",
        "        #Concat u with u1, and m with m1. Compute the dot product of the resulting vectors, and pass the value through a sigmoid.\n",
        "\n",
        "        #If using the model with BOW and BCE\n",
        "        # return F.cosine_similarity(u1,m1)\n",
        "class ToxicityBERTWithEmbeddings(nn.Module):\n",
        "    def __init__(self,d):\n",
        "        super(ToxicityBERTWithEmbeddings,self).__init__()\n",
        "\n",
        "        #We only have one trainable layer depth (two Embeddings and two FC's)\n",
        "\n",
        "        self.u1 = nn.Linear(bertavg_users.shape[1],d)       #FC        author bow -> n_factors\n",
        "        self.m1 = nn.Linear(bertavg_subreddits.shape[1],d)  #FC        subreddit bow -> n_factors\n",
        "\n",
        "        self.u = nn.Embedding(n_users,d)                #Embedding author_id -> n_factors\n",
        "        self.m = nn.Embedding(n_subreddits,d)           #Embedding subreddit_id-> n_factors\n",
        "        \n",
        "        self.fc1 = nn.Linear(2*d, d//2)\n",
        "\n",
        "        self.fc_11 = nn.Linear(d, d//2)\n",
        "\n",
        "        self.final = nn.Linear(d,1)\n",
        "        #Initialize weights\n",
        "        # self.u1.weight.data.uniform_(-0.1, 0.1)\n",
        "        # self.m1.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ubert, sbert = bertavg_users[x[:,0].long()], bertavg_subreddits[x[:, 1].long()] #Get author bow and subreddit bow indirectly from input\n",
        "        u1, m1 = self.u1(ubert), self.m1(sbert) #Reduce dimensionality of author bow and subreddit bow\n",
        "\n",
        "\n",
        "        conc=torch.cat((u1,m1),1)\n",
        "        conc=F.dropout(F.relu(conc),0.1)\n",
        "        conc = F.dropout(F.relu(self.fc1(conc)),0.1)\n",
        "        \n",
        "        u, m = self.u(x[:,0]), self.m(x[:, 1])\n",
        "        mul = F.dropout(F.relu(self.fc_11(u*m)),0.1)\n",
        "\n",
        "        conc = self.final(torch.cat((conc,mul),1))\n",
        "\n",
        "\n",
        "\n",
        "        return conc\n",
        "        #Concat u with u1, and m with m1. Compute the dot product of the resulting vectors, and pass the value through a sigmoid.\n",
        "\n",
        "        #If using the model with BOW and BCE\n",
        "        # return F.cosine_similarity(u1,m1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# One-class SVM tests\n",
        "#\n",
        "\n",
        "# from sklearn.svm import OneClassSVM\n",
        "\n",
        "# x_trainusers_bert = bertavg_users[X_train.cpu().numpy()[:,0]]\n",
        "# x_trainsubs_bert = bertavg_subreddits[X_train.cpu().numpy()[:,1]]\n",
        "\n",
        "# svm_train_set = np.concatenate((x_trainusers_bert.cpu().numpy(),x_trainsubs_bert.cpu().numpy()),axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectPercentile, mutual_info_regression\n",
        "\n",
        "# print(svm_train_set.shape)\n",
        "# #Ref: https://medium.com/analytics-vidhya/feature-selection-using-scikit-learn-5b4362e0c19b\n",
        "# #Ref: https://towardsdatascience.com/5-feature-selection-method-from-scikit-learn-you-should-know-ed4d116e4172\n",
        "# selector = SelectPercentile(mutual_info_regression, percentile=20)\n",
        "# selector.fit(svm_train_set,y_train.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# #Reference: https://www.datatechnotes.com/2020/04/anomaly-detection-with-one-class-svm.html\n",
        "# selected_chars = np.squeeze(svm_train_set[:,np.where(selector.get_support()==True)])\n",
        "# print(selected_chars.shape)\n",
        "\n",
        "# svm = OneClassSVM(kernel='rbf', gamma='scale', nu=1/toxic_labels_weight, verbose=True, cache_size=1600)\n",
        "# svm.fit(selected_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predictions = svm.predict(selected_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predictions[predictions==1]=0\n",
        "# predictions[predictions==-1]=1\n",
        "\n",
        "# labels=y_train.cpu().numpy()\n",
        "\n",
        "# tp = np.sum(np.logical_and(predictions,labels))\n",
        "# tn = np.sum(np.logical_and(np.logical_not(predictions),np.logical_not(labels)))\n",
        "# fn = np.sum(np.logical_and(np.logical_not(predictions),labels))\n",
        "# fp = np.sum(np.logical_and(predictions,np.logical_not(labels)))\n",
        "\n",
        "# print(tp, fp, fn, tn)\n",
        "\n",
        "# print(f\"Accuracy: {(tn+tp)/(tn+tp+fn+fp)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [],
      "source": [
        "# x_testusers_bert = bertavg_users[X_test.cpu().numpy()[:,0]]\n",
        "# x_testsubs_bert = bertavg_subreddits[X_test.cpu().numpy()[:,1]]\n",
        "\n",
        "# svm_test_set = (x_testusers_bert.cpu()*x_testsubs_bert.cpu()).numpy()\n",
        "# selected_chars_test = np.squeeze(svm_test_set[:,np.where(selector.get_support()==True)])\n",
        "\n",
        "# predictions_test = svm.predict(selected_chars_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [],
      "source": [
        "# predictions=predictions_test.copy()\n",
        "\n",
        "\n",
        "# print(predictions.shape,labels.shape)\n",
        "# predictions[predictions==1]=0\n",
        "# predictions[predictions==-1]=1\n",
        "\n",
        "# labels=y_test.cpu().numpy()\n",
        "\n",
        "# tp = np.sum(np.logical_and(predictions,labels))\n",
        "# tn = np.sum(np.logical_and(np.logical_not(predictions),np.logical_not(labels)))\n",
        "# fn = np.sum(np.logical_and(np.logical_not(predictions),labels))\n",
        "# fp = np.sum(np.logical_and(predictions,np.logical_not(labels)))\n",
        "\n",
        "# print(tp, fp, fn, tn)\n",
        "\n",
        "# print(f\"Accuracy: {(tn+tp)/(tn+tp+fn+fp)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "fASFMYWrUG0y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toxic interactions (positive samples) in train test: 49.99%\n",
            "Toxic interactions (positive samples) in test test: 50.00%\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Create the Dataset and Dataloader objects for train and test sets\n",
        "#\n",
        "\n",
        "class ToxicityDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x=X_train\n",
        "        self.y=y_train\n",
        "        self.n_samples=X_train.size(0)\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index],self.y[index]\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x=X_test\n",
        "        self.y=y_test\n",
        "        self.n_samples=X_test.size(0)\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index],self.y[index]\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "train_data=ToxicityDataset()\n",
        "dataloader=DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "test_data=TestDataset()\n",
        "test_dataloader=DataLoader(dataset=test_data,batch_size=BATCH_SIZE,shuffle=True)\n",
        "\n",
        "print(f\"Toxic interactions (positive samples) in train test: {np.average((y_train>0).cpu())*100:.2f}%\")\n",
        "print(f\"Toxic interactions (positive samples) in test test: {np.average((y_test>0).cpu())*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "zS4VQMToUG0y",
        "outputId": "7067110c-13fb-45af-b8b7-3a604204c28e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA availability: True\n",
            "Using pytorch version 1.12.1+cu113, 11.3\n"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Define the training cycle for the model\n",
        "#\n",
        "print(f\"CUDA availability: {torch.cuda.is_available()}\")\n",
        "print(f\"Using pytorch version {torch.__version__}, {torch.version.cuda}\")\n",
        "def train_toxicity_model(n_factors=64,learning_rate=1e-3,l2_reg=1e-5,epochs=10):\n",
        "    print(f\"Current parameters are d:{n_factors} | L.Rate:{learning_rate} | L2 Reg:{l2_reg}\")\n",
        "    \n",
        "    losses=[]\n",
        "    losses_test=[]\n",
        "    accuracies=[]\n",
        "    accuracies_test=[]\n",
        "    TPRs=[]\n",
        "    TPRs_test=[]\n",
        "    \n",
        "    Iterations=[]\n",
        "\n",
        "    #Initialize model\n",
        "    if MODEL_ARCHITECTURE==\"toxicity_simple\":\n",
        "        model = ToxicitySimple(n_factors).to(device)\n",
        "    elif MODEL_ARCHITECTURE==\"toxicity_NCF\":\n",
        "        model = ToxicityNCF(n_factors).to(device)\n",
        "    elif MODEL_ARCHITECTURE==\"toxicity_BOW\":\n",
        "        model = ToxicityBOW(n_factors).to(device)\n",
        "    elif MODEL_ARCHITECTURE==\"toxicity_BERT_simple\":\n",
        "        model = ToxicityBERTSimple(n_factors).to(device)\n",
        "    elif MODEL_ARCHITECTURE==\"toxicity_BERTwithembeddings\":\n",
        "        model = ToxicityBERTWithEmbeddings(n_factors).to(device)\n",
        "    #Model configuration\n",
        "    optimizer=torch.optim.Adam(model.parameters(), learning_rate, weight_decay=l2_reg) #Weight_decay acts as L2 regularization apparently\n",
        "\n",
        "    print(\"EPOCH\\tLOSS_TRAIN\\tLOSS_TEST\\tACC_TRAIN\\tACC_TEST\\tTP\\tFP\\tFN\\tTN\\tTPR_TRAIN\\tTPR_TEST\")\n",
        "    # for epoch in tqdm(range(int(epochs)), desc='Training Epochs'):\n",
        "    for epoch in range(int(epochs)):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        loss_train=0\n",
        "        \n",
        "        fn=0\n",
        "        fp=0\n",
        "        tn=0\n",
        "        tp=0\n",
        "\n",
        "        #Iterate training over train batches\n",
        "        for i,(inputs,labels) in enumerate(dataloader):\n",
        "            optimizer.zero_grad()  # Setting our stored gradients equal to zero\n",
        "            outputs = torch.squeeze(model(inputs))\n",
        "\n",
        "            #References: https://stackoverflow.com/questions/71462326/pytorch-bcewithlogitsloss-calculating-pos-weight\n",
        "            # https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586 (apparently more numerically stable than BCELoss)\n",
        "\n",
        "            if TRAINING_GOAL==\"classification\":\n",
        "                criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([toxic_labels_weight]).to(device))\n",
        "            elif TRAINING_GOAL==\"regression\":\n",
        "                criterion = torch.nn.MSELoss()\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss_train += loss.item()*inputs.size(0)\n",
        "\n",
        "            # print(loss)\n",
        "            \n",
        "            \n",
        "\n",
        "            loss.backward()  # Computes the gradient of the given tensor w.r.t. the weights/bias\n",
        "\n",
        "            optimizer.step()  # Updates weights and biases with the optimizer (Adam)\n",
        "\n",
        "            #https://discuss.pytorch.org/t/bcewithlogitsloss-and-model-accuracy-calculation/59293\n",
        "            predicted_train = ((outputs.cpu() > 0.0)).float().detach().numpy()\n",
        "            \n",
        "            labels_train = ((labels.cpu() > 0.0)).detach().cpu().numpy()\n",
        "\n",
        "\n",
        "            tp += np.sum(np.logical_and(predicted_train,labels_train))\n",
        "            tn += np.sum(np.logical_and(np.logical_not(predicted_train),np.logical_not(labels_train)))\n",
        "            fn += np.sum(np.logical_and(np.logical_not(predicted_train),labels_train))\n",
        "            fp += np.sum(np.logical_and(predicted_train,np.logical_not(labels_train)))\n",
        "\n",
        "        TPRs.append(100*tp/(tp+fn))\n",
        "                \n",
        "        accuracy = 100 * (tp+tn) / y_train.size(0)\n",
        "        \n",
        "        loss_train = loss_train/y_train.size(0)\n",
        "            \n",
        "        losses.append(loss_train)\n",
        "        accuracies.append(accuracy)\n",
        "    \n",
        "           \n",
        "        with torch.no_grad():\n",
        "            # Compute metrics for test dataset\n",
        "            model.eval()\n",
        "            loss_test=0\n",
        "\n",
        "            fn=0\n",
        "            fp=0\n",
        "            tn=0\n",
        "            tp=0\n",
        "\n",
        "            for j, (test_inputs, test_labels) in enumerate(test_dataloader):\n",
        "                \n",
        "                outputs_test = torch.squeeze(model(test_inputs))\n",
        "\n",
        "                if TRAINING_GOAL==\"classification\":\n",
        "                    if TRAIN_TEST_METHOD==\"random\":\n",
        "                        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([toxic_labels_weight]).to(device))\n",
        "                    elif TRAIN_TEST_METHOD in [\"controlled_users\",\"controlled_subreddits\",\"controlled_both\"]:\n",
        "                        criterion = torch.nn.BCEWithLogitsLoss()\n",
        "                elif TRAINING_GOAL==\"regression\":\n",
        "                        criterion = torch.nn.MSELoss()\n",
        "\n",
        "                loss_test += criterion(outputs_test, test_labels).item()*test_inputs.size(0)\n",
        "\n",
        "                predicted_test = ((outputs_test.detach().cpu() > 0.0)).numpy()\n",
        "\n",
        "                labels_test = ((test_labels.detach().cpu() > 0.0)).numpy()\n",
        "\n",
        "                tp += np.sum(np.logical_and(predicted_test,labels_test))\n",
        "                tn += np.sum(np.logical_and(np.logical_not(predicted_test),np.logical_not(labels_test)))\n",
        "                fn += np.sum(np.logical_and(np.logical_not(predicted_test),labels_test))\n",
        "                fp += np.sum(np.logical_and(predicted_test,np.logical_not(labels_test)))\n",
        "\n",
        "            accuracy_test = 100 * (tn+tp) / y_test.size(0)\n",
        "\n",
        "            loss_test = loss_test/y_test.size(0)\n",
        "\n",
        "            losses_test.append(loss_test)\n",
        "            accuracies_test.append(accuracy_test)\n",
        "\n",
        "            # Calculating the loss and accuracy for the train dataset.\n",
        "\n",
        "            Iterations.append(iter)\n",
        "\n",
        "            TPRs_test.append(100*tp/(tp+fn))\n",
        "\n",
        "            # avg_1_train=np.average(torch.squeeze(outputs).cpu()[np.where(labels_train==1)])\n",
        "            # avg_1_test=np.average(outputs_test[np.where(labels_test==1)])\n",
        "            \n",
        "            print(f\"{epoch}\\t{loss_train:.7f}\\t{loss_test:.7f}\\t{accuracy:.2f}\\t\\t{accuracy_test:.2f}\\t\\t{tp}\\t{fp}\\t{fn}\\t{tn}\\t{TPRs[-1]:.2f}\\t\\t{TPRs_test[-1]:.2f}\",end=\"\\r\")\n",
        "    \n",
        "    print(f\"{epoch}\\t{loss_train:.7f}\\t{loss_test:.7f}\\t{accuracy:.2f}\\t\\t{accuracy_test:.2f}\\t\\t{tp}\\t{fp}\\t{fn}\\t{tn}\\t{TPRs[-1]:.2f}\\t{TPRs_test[-1]:.2f}\")\n",
        "    return losses,losses_test,accuracies,accuracies_test, TPRs, TPRs_test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "grid_search/toxicity_simple_splitcontrolled_both_balancetrainTrue_1666218970.7809894\n",
            "Current parameters are d:32 | L.Rate:0.001 | L2 Reg:0.0005\n",
            "EPOCH\tLOSS_TRAIN\tLOSS_TEST\tACC_TRAIN\tACC_TEST\tTP\tFP\tFN\tTN\tTPR_TRAIN\tTPR_TEST\n",
            "77\t6.6244010\t18.5400620\t91.82\t\t49.47\t\t578\t590\t547\t535\t88.26\t\t51.383\r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [201], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m lr \u001b[39min\u001b[39;00m learning_rates:\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m reg \u001b[39min\u001b[39;00m l2_reg:\n\u001b[1;32m---> 19\u001b[0m         losses,losses_test,accuracies,accuracies_test,TPRs,TPRs_test \u001b[39m=\u001b[39m train_toxicity_model(n_factors\u001b[39m=\u001b[39;49md,learning_rate\u001b[39m=\u001b[39;49mlr,l2_reg\u001b[39m=\u001b[39;49mreg,epochs\u001b[39m=\u001b[39;49mepochs)\n\u001b[0;32m     21\u001b[0m         \u001b[39m#Plot current training interation:\u001b[39;00m\n\u001b[0;32m     22\u001b[0m         plt\u001b[39m.\u001b[39msubplot(\u001b[39mlen\u001b[39m(learning_rates),\u001b[39mlen\u001b[39m(l2_reg),i)\n",
            "Cell \u001b[1;32mIn [200], line 46\u001b[0m, in \u001b[0;36mtrain_toxicity_model\u001b[1;34m(n_factors, learning_rate, l2_reg, epochs)\u001b[0m\n\u001b[0;32m     43\u001b[0m tp\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m     45\u001b[0m \u001b[39m#Iterate training over train batches\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfor\u001b[39;00m i,(inputs,labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     47\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Setting our stored gradients equal to zero\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(model(inputs))\n",
            "File \u001b[1;32md:\\TFM\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32md:\\TFM\\venv39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32md:\\TFM\\venv39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32md:\\TFM\\venv39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn [197], line 11\u001b[0m, in \u001b[0;36mToxicityDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m---> 11\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx[index],\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my[index]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 2000x2000 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#\n",
        "# Perform grid search\n",
        "#\n",
        "\n",
        "# Figures are saved in pdf format in a folder specific to configuration. One figure is created \n",
        "# per each *d* used, with one subfigure per each (lr, L2 reg) combination\n",
        "\n",
        "directory_path = f\"grid_search/{MODEL_ARCHITECTURE}_split{TRAIN_TEST_METHOD}_balancetrain{BALANCE_TRAIN_USERS}_{time.time()}\"\n",
        "print(directory_path)\n",
        "os.makedirs(directory_path,exist_ok=True)\n",
        "\n",
        "for d in n_factors:\n",
        "    i=1\n",
        "\n",
        "    plt.figure(figsize=(20,(20*min(len(learning_rates),len(l2_reg)))/max(len(learning_rates),len(l2_reg))))\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for reg in l2_reg:\n",
        "            losses,losses_test,accuracies,accuracies_test,TPRs,TPRs_test = train_toxicity_model(n_factors=d,learning_rate=lr,l2_reg=reg,epochs=epochs)\n",
        "\n",
        "            #Plot current training interation:\n",
        "            plt.subplot(len(learning_rates),len(l2_reg),i)\n",
        "            plt.title(f\"d={d} | lr={lr} | l2-reg={reg}\",fontdict={'fontsize': 12})\n",
        "\n",
        "            plt.xticks(np.arange(0,epochs+1,20),fontsize=12)\n",
        "            plt.yticks(np.arange(0,5,0.5),fontsize=12)\n",
        "\n",
        "            plt.ylim(0,5)\n",
        "            \n",
        "            plt.plot(np.arange(0,epochs,1),losses, color=\"red\",alpha=.25,label=\"Train Loss\") #Train loss evolution\n",
        "            plt.plot(np.arange(0,epochs,1),losses_test, color=\"blue\",alpha=.25,label=\"Test Loss\") #Test loss evolution\n",
        "\n",
        "            if i==1: plt.legend(loc=\"upper left\")\n",
        "\n",
        "            plt.twinx() #Swap axis\n",
        "\n",
        "            plt.yticks(np.arange(30,101,10),fontsize=12)\n",
        "\n",
        "            plt.ylim(30,100)\n",
        "\n",
        "            plt.plot(np.arange(0,epochs,1),accuracies, color=\"red\", label=\"Train Acc\")         #Train acc evolution\n",
        "            plt.plot(np.arange(0,epochs,1),accuracies_test, color=\"blue\", label=\"Test Acc\")   #Test acc evolution\n",
        "\n",
        "            plt.plot(np.arange(0,epochs,1),TPRs, '--', color=\"red\", label=\"Train TPR (%)\" , alpha=.3)   # Train TPR (%) evolution\n",
        "            plt.plot(np.arange(0,epochs,1),TPRs_test, '--', color=\"blue\", label=\"Test TPR (%)\", alpha=.3)   #Test TPR (%) evolution\n",
        "\n",
        "            if i==1: plt.legend(loc=\"center left\")\n",
        "            i+=1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{directory_path}/d_{d}.pdf\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('venv39': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "74f696c15222280786284ce7e7866cfb85e8efe029d78b7ee203451221269bd4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
